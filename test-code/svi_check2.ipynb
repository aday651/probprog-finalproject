{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport bitcoin\n",
    "import torch\n",
    "import pyro\n",
    "import time\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "import pyro.poutine as poutine\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.distributions import constraints\n",
    "from bitcoin import BitcoinOTC\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "\n",
    "# Validation checks\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrew\\anaconda3\\envs\\ppp\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\andrew\\anaconda3\\envs\\ppp\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\users\\andrew\\anaconda3\\envs\\ppp\\lib\\site-packages\\numpy\\core\\_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "c:\\users\\andrew\\anaconda3\\envs\\ppp\\lib\\site-packages\\numpy\\core\\_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "c:\\users\\andrew\\anaconda3\\envs\\ppp\\lib\\site-packages\\numpy\\core\\_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "data = bitcoin.BitcoinOTC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_scale = 1.0\n",
    "obs_scale = 1.0\n",
    "embed_dim = 2\n",
    "\n",
    "def guide(data, node_ind, edge_ind, edge_list):\n",
    "    r\"\"\"Defines a variational family to use to fit an approximate posterior\n",
    "    distribution for the probability model defined in model.\n",
    "    \"\"\"\n",
    "    # Deleting arguments not used in the guide for linting purposes\n",
    "    del edge_list\n",
    "    \n",
    "    # Parameters governing the priors on the embedding vectors\n",
    "    # omega_loc should have shape [embed_dim, data.num_nodes]\n",
    "    omega_loc = pyro.param('omega_loc', lambda: 0.5*torch.randn(embed_dim, data.num_nodes))\n",
    "    # omega_scale should be a single positive tensor\n",
    "    omega_scale = pyro.param('omega_scale', torch.tensor(1.0), constraint=constraints.positive)\n",
    "\n",
    "    # Parameters governing the prior for the linear regression\n",
    "    # beta_loc should be of shape [embed_dim]\n",
    "    beta_loc = pyro.param('beta_loc', 0.5*torch.randn(embed_dim))\n",
    "    # beta_scale should be a single positive tensor\n",
    "    beta_scale = pyro.param('beta_scale', torch.tensor(1.0), constraint=constraints.positive)\n",
    "    # mu_loc should be a single tensor\n",
    "    mu_loc = pyro.param('mu_loc', torch.randn(1))\n",
    "    # mu_scale should be a single positive tensor\n",
    "    mu_scale = pyro.param('mu_scale', torch.tensor(1.0), constraint=constraints.positive)\n",
    "\n",
    "    # Sample the coefficient vector and intercept for linear regression\n",
    "    beta = pyro.sample('beta', dist.Normal(loc=beta_loc, scale=beta_scale*torch.ones(embed_dim)))\n",
    "    mu = pyro.sample('mu', dist.Normal(mu_loc, mu_scale))\n",
    "\n",
    "    # Subsample embedding vectors - this is going to be fun...\n",
    "    with pyro.plate('nodes', size=data.num_nodes, subsample=node_ind) as plate_node_ind:\n",
    "        omega = pyro.sample('omega', dist.Normal(\n",
    "                                        loc=omega_loc,\n",
    "                                        scale=omega_scale*torch.ones(embed_dim, data.num_nodes)).to_event(-1)\n",
    "        ).index_select(-1, plate_node_ind)\n",
    "\n",
    "    # Define plate for the edge subsampling to pass to model object\n",
    "    with pyro.plate('edges', size=data.num_edges, subsample=edge_ind) as plate_edge_ind:\n",
    "        # Note: we use pyro plate here as we need to keep the subsampling\n",
    "        # persistent in both our model and guide functions, we so we\n",
    "        # define an empty plate here just so we can use the same call\n",
    "        # in the model.\n",
    "        pass\n",
    "\n",
    "    return omega, beta, mu\n",
    "\n",
    "\n",
    "def model(data, node_ind, edge_ind, edge_list):\n",
    "    r\"\"\"Defines a probabilistic model for the observed network data.\"\"\"\n",
    "    # Define priors on the regression coefficients\n",
    "    mu = pyro.sample('mu', dist.Normal(torch.tensor(0.0), torch.tensor(2.0)))\n",
    "    beta = pyro.sample('beta', dist.MultivariateNormal(\n",
    "        loc=torch.zeros(embed_dim), covariance_matrix=4*torch.eye(embed_dim)\n",
    "    ))\n",
    "\n",
    "    # Define prior on the embedding vectors, do subsampling for the\n",
    "    # embedding vector and then the likelihood term for the observed nodes\n",
    "    with pyro.plate('nodes', size=data.num_nodes) as plate_node_ind:\n",
    "        # Embedding vectors of shape [embed_dim, data.num_nodes]\n",
    "        omega = pyro.sample('omega', \n",
    "                            dist.Normal(\n",
    "                                loc=torch.zeros(embed_dim, data.num_nodes),\n",
    "                                scale=omega_scale*torch.ones(embed_dim, data.num_nodes)).to_event(-1)\n",
    "                            ).index_select(-1, plate_node_ind)\n",
    "\n",
    "        # Draw Bernoulli, with or without data depending on if it is observed\n",
    "        logit_prob = mu + torch.mv(omega.t(), beta)\n",
    "        \n",
    "        # Create mask corresponding to entries of ind which lie within data.nodes_train\n",
    "        obs_mask = np.isin(ind_nodes.numpy(), data.nodes_train.numpy()).tolist()\n",
    "        valid_data = data.gt[plate_node_ind].clone()\n",
    "        valid_data[~obs_mask] = 0\n",
    "        with poutine.mask(mask=obs_mask):\n",
    "            pyro.sample('trust', dist.Bernoulli(logits=logit_prob), obs=valid_data)\n",
    "\n",
    "    # Begin extracting the relevant components of the gram matrix formed by omega now\n",
    "    # Note that to extract the relevant indices, we need to account for the\n",
    "    # change in indexing induced by subsampling omega\n",
    "    gram = torch.mm(omega.t(), omega)\n",
    "    t = torch.zeros(node_ind.max() + 1, dtype=torch.long)\n",
    "    t[node_ind] = torch.arange(len(node_ind))\n",
    "    gram_sample = gram[t[edge_list[0, :]], t[edge_list[0, :]]]\n",
    "        \n",
    "    # Draw terms corresponding to the edges\n",
    "    with pyro.plate('edges', size=data.num_edges, subsample=edge_ind):\n",
    "        pyro.sample('a', dist.Normal(loc=gram_sample, scale=obs_scale*torch.ones(len(edge_ind))), obs=data.edge_weight_logit[edge_ind])\n",
    "        \n",
    "\n",
    "# Define SVI object\n",
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({\"lr\": .05}),\n",
    "          loss=TraceGraph_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Shapes:             \n",
      " Param Sites:             \n",
      "    omega_loc 2 6005      \n",
      "  omega_scale             \n",
      "     beta_loc      2      \n",
      "   beta_scale             \n",
      "       mu_loc      1      \n",
      "     mu_scale             \n",
      "Sample Sites:             \n",
      "    beta dist      | 2    \n",
      "        value      | 2    \n",
      "     log_prob      |      \n",
      "      mu dist      | 1    \n",
      "        value      | 1    \n",
      "     log_prob      |      \n",
      "   omega dist      | 2 297\n",
      "        value      | 2 297\n",
      "     log_prob      |      \n"
     ]
    }
   ],
   "source": [
    "# Define model object\n",
    "def guide(data, node_ind, edge_ind, edge_list):\n",
    "    r\"\"\"Defines a variational family to use to fit an approximate posterior\n",
    "    distribution for the probability model defined in model.\"\"\"\n",
    "    ## Deleting arguments not used in the guide for linting purposes\n",
    "    del edge_ind, edge_list\n",
    "    \n",
    "    ## Parameters governing the priors on the embedding vectors\n",
    "    # omega_loc should have shape [embed_dum, data.num_nodes]\n",
    "    omega_loc = pyro.param('omega_loc',\n",
    "                           lambda: 0.5*torch.randn(embed_dim, data.num_nodes))\n",
    "    # omega_scale should be a single positive tensor\n",
    "    omega_scale = pyro.param('omega_scale',\n",
    "                             torch.tensor(1.0),\n",
    "                             constraint=constraints.positive)\n",
    "    \n",
    "    ## Paramaeters governing the prior fr the linear regression\n",
    "    # beta_loc should be of shape [embed_dim]\n",
    "    beta_loc = pyro.param('beta_loc',\n",
    "                          0.5*torch.randn(embed_dim))\n",
    "    # beta_scale should be a single positive tensor\n",
    "    beta_scale = pyro.param('beta_scale',\n",
    "                            torch.tensor(1.0),\n",
    "                            constraint=constraints.positive)\n",
    "    # mu_loc should be a single tensor\n",
    "    mu_loc = pyro.param('mu_loc',\n",
    "                        torch.randn(1))\n",
    "    # mu_scale should be a single positive tensor\n",
    "    mu_scale = pyro.param('mu_scale',\n",
    "                          torch.tensor(1.0),\n",
    "                          constraint=constraints.positive)\n",
    "    \n",
    "    ## Sample the coefficient vector and intercept for linear regression\n",
    "    beta = pyro.sample('beta',\n",
    "                       dist.Normal(loc=beta_loc, \n",
    "                                   scale=beta_scale*torch.ones(embed_dim)).to_event(1))\n",
    "    mu = pyro.sample('mu',\n",
    "                     dist.Normal(mu_loc, mu_scale).to_event(1))\n",
    "    \n",
    "    ## Handle the subsampling of the embedding vectors\n",
    "    with poutine.scale(scale=data.num_nodes/len(node_ind)):\n",
    "        omega = pyro.sample('omega', \n",
    "                            dist.Normal(loc=omega_loc[:, node_ind],\n",
    "                                        scale=omega_scale).to_event(2))\n",
    "    \n",
    "subsample_dict = data.subsample(**sample_args_dict)\n",
    "trace = poutine.trace(guide).get_trace(data, **subsample_dict)\n",
    "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Shapes:          \n",
      " Param Sites:          \n",
      "Sample Sites:          \n",
      "      mu dist |   1    \n",
      "        value |   1    \n",
      "     log_prob |        \n",
      "    beta dist |   2    \n",
      "        value |   2    \n",
      "     log_prob |        \n",
      "   omega dist |   2 270\n",
      "        value |   2 270\n",
      "     log_prob |        \n",
      "   trust dist |  28    \n",
      "        value |  28    \n",
      "     log_prob |        \n",
      "       a dist | 166    \n",
      "        value | 166    \n",
      "     log_prob |        \n"
     ]
    }
   ],
   "source": [
    "omega_model_scale = 1.0\n",
    "obs_scale = 1.0\n",
    "embed_dim = 2\n",
    "\n",
    "def model(data, node_ind, edge_ind, edge_list):\n",
    "    r\"\"\"Defines a probabilistic model for the observed network data.\"\"\"\n",
    "    # Define priors on the regression coefficients\n",
    "    mu = pyro.sample('mu',\n",
    "                     dist.Normal(torch.tensor([0.0]), \n",
    "                                 torch.tensor([2.0])).to_event(1))\n",
    "    \n",
    "    beta = pyro.sample('beta',\n",
    "                       dist.Normal(loc=torch.zeros(embed_dim),\n",
    "                                   scale=torch.tensor(2.0)).to_event(1))\n",
    "    \n",
    "    # Define prior on the embedding vectors, with subsampling\n",
    "    with poutine.scale(scale=data.num_nodes/len(node_ind)):\n",
    "        omega = pyro.sample('omega',\n",
    "                            dist.Normal(loc=torch.zeros(embed_dim, len(node_ind)),\n",
    "                                        scale=omega_model_scale).to_event(2))\n",
    "    \n",
    "    # Before proceeding further, define a list t which acts as the inverse\n",
    "    # function of node_ind - i.e it takes a number in node_ind to its\n",
    "    # index location\n",
    "    t = torch.zeros(node_ind.max() + 1, dtype=torch.long)\n",
    "    t[node_ind] = torch.arange(len(node_ind))\n",
    "    \n",
    "    # Create mask corresponding to entries of ind which lie within the\n",
    "    # training set (i.e data.train_nodes)\n",
    "    gt_data = data.gt[node_ind]\n",
    "    obs_mask = np.isin(node_ind, data.nodes_train).tolist()\n",
    "    gt_data[gt_data != gt_data] = 0.0\n",
    "    obs_mask = torch.tensor(obs_mask, dtype=torch.bool)\n",
    "    \n",
    "    # Compute logits, compute relevant parts of sample\n",
    "    if sum(obs_mask) != 0:\n",
    "        logit_prob = mu + torch.mv(omega.t(), beta)\n",
    "        with poutine.scale(scale=len(data.nodes_train)/sum(obs_mask)):\n",
    "            pyro.sample('trust',\n",
    "                        dist.Bernoulli(logits=logit_prob[obs_mask]).independent(1),\n",
    "                        obs=gt_data[obs_mask])\n",
    "    \n",
    "    # Begin extracting the relevant components of the gram matrix formed by omega now\n",
    "    # Note that to extract the relevant indices, we need to account for the\n",
    "    # change in indexing induced by subsampling omega\n",
    "    gram = torch.mm(omega.t(), omega)\n",
    "    gram_sample = gram[t[edge_list[0, :]], t[edge_list[0, :]]]\n",
    "    \n",
    "    # Finally draw terms corresponding to the edges\n",
    "    with poutine.scale(scale=data.num_edges/len(edge_ind)):\n",
    "        pyro.sample('a', dist.Normal(loc=gram_sample, scale=obs_scale).to_event(1),\n",
    "                    obs=data.edge_weight_logit[edge_ind])\n",
    "\n",
    "subsample_dict = data.subsample(**sample_args_dict)\n",
    "trace = poutine.trace(model).get_trace(data, **subsample_dict)\n",
    "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SVI object\n",
    "svi = SVI(model,\n",
    "          guide,\n",
    "          optim.Adam({\"lr\": .05}),\n",
    "          loss=TraceGraph_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elbo loss: 334435.5\n",
      "Elbo loss: 137262.75\n",
      "Time to run 5 gradient steps of SVI: 0.7655518054962158\n",
      "Elbo loss: 90147.3828125\n",
      "Time to run 5 gradient steps of SVI: 0.6839547157287598\n",
      "Elbo loss: 85527.5234375\n",
      "Time to run 5 gradient steps of SVI: 0.7819507122039795\n",
      "Elbo loss: 67257.734375\n",
      "Time to run 5 gradient steps of SVI: 0.6419391632080078\n",
      "Elbo loss: 72872.8359375\n",
      "Time to run 5 gradient steps of SVI: 0.6312680244445801\n",
      "Elbo loss: 67029.578125\n",
      "Time to run 5 gradient steps of SVI: 0.6811816692352295\n",
      "Elbo loss: 67685.296875\n",
      "Time to run 5 gradient steps of SVI: 0.6515693664550781\n",
      "Elbo loss: 60318.62109375\n",
      "Time to run 5 gradient steps of SVI: 0.6661176681518555\n",
      "Elbo loss: 62692.75\n",
      "Time to run 5 gradient steps of SVI: 0.6082806587219238\n",
      "Elbo loss: 59708.640625\n",
      "Time to run 5 gradient steps of SVI: 0.6349401473999023\n",
      "Elbo loss: 59145.921875\n",
      "Time to run 5 gradient steps of SVI: 0.6262016296386719\n",
      "Elbo loss: 69573.078125\n",
      "Time to run 5 gradient steps of SVI: 0.6039650440216064\n",
      "Elbo loss: 69402.9765625\n",
      "Time to run 5 gradient steps of SVI: 0.651940107345581\n",
      "Elbo loss: 60495.43359375\n",
      "Time to run 5 gradient steps of SVI: 0.7170336246490479\n",
      "Elbo loss: 61312.84375\n",
      "Time to run 5 gradient steps of SVI: 0.6625940799713135\n",
      "Elbo loss: 58961.1171875\n",
      "Time to run 5 gradient steps of SVI: 0.6514687538146973\n",
      "Elbo loss: 56202.203125\n",
      "Time to run 5 gradient steps of SVI: 0.6901304721832275\n",
      "Elbo loss: 56717.9375\n",
      "Time to run 5 gradient steps of SVI: 0.7048492431640625\n",
      "Elbo loss: 62720.23046875\n",
      "Time to run 5 gradient steps of SVI: 0.6682538986206055\n",
      "Elbo loss: 64396.23828125\n",
      "Time to run 5 gradient steps of SVI: 0.5937602519989014\n",
      "Elbo loss: 68386.4453125\n",
      "Time to run 5 gradient steps of SVI: 0.6381285190582275\n",
      "Elbo loss: 58186.8515625\n",
      "Time to run 5 gradient steps of SVI: 0.6725780963897705\n",
      "Elbo loss: 60005.53125\n",
      "Time to run 5 gradient steps of SVI: 0.5948491096496582\n",
      "Elbo loss: 60075.60546875\n",
      "Time to run 5 gradient steps of SVI: 0.6849803924560547\n",
      "Elbo loss: 69533.5859375\n",
      "Time to run 5 gradient steps of SVI: 0.6605045795440674\n",
      "Elbo loss: 63694.40625\n",
      "Time to run 5 gradient steps of SVI: 0.6568899154663086\n",
      "Elbo loss: 57786.92578125\n",
      "Time to run 5 gradient steps of SVI: 0.5681405067443848\n",
      "Elbo loss: 56666.72265625\n",
      "Time to run 5 gradient steps of SVI: 0.6552734375\n",
      "Elbo loss: 61798.984375\n",
      "Time to run 5 gradient steps of SVI: 0.6298906803131104\n",
      "Elbo loss: 56036.1015625\n",
      "Time to run 5 gradient steps of SVI: 0.7000918388366699\n",
      "Elbo loss: 58554.55078125\n",
      "Time to run 5 gradient steps of SVI: 0.5692508220672607\n",
      "Elbo loss: 54868.30078125\n",
      "Time to run 5 gradient steps of SVI: 0.6204378604888916\n",
      "Elbo loss: 61495.4765625\n",
      "Time to run 5 gradient steps of SVI: 0.622917652130127\n",
      "Elbo loss: 60735.9921875\n",
      "Time to run 5 gradient steps of SVI: 0.6658322811126709\n",
      "Elbo loss: 61205.296875\n",
      "Time to run 5 gradient steps of SVI: 0.6572222709655762\n",
      "Elbo loss: 58359.6328125\n",
      "Time to run 5 gradient steps of SVI: 0.5604844093322754\n",
      "Elbo loss: 63831.34375\n",
      "Time to run 5 gradient steps of SVI: 0.5999939441680908\n",
      "Elbo loss: 65743.3515625\n",
      "Time to run 5 gradient steps of SVI: 0.632129430770874\n",
      "Elbo loss: 60079.30859375\n",
      "Time to run 5 gradient steps of SVI: 0.6414201259613037\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "pyro.clear_param_store()\n",
    "num_iters = 200\n",
    "sample_args_dict = {'sample_str': 'p-sampling', 'sample_args': {'sample_prob': 0.005}}\n",
    "\n",
    "for i in range(num_iters):\n",
    "    subsample_dict = data.subsample(**sample_args_dict)\n",
    "    elbo = svi.step(data, **subsample_dict)\n",
    "    if i % 5 == 0:\n",
    "        logging.info(\"Elbo loss: {}\".format(elbo))\n",
    "        if (i > 0):\n",
    "            t1 = time.time()\n",
    "            logging.info(\"Time to run 5 gradient steps of SVI: {}\".format(t1-t0))\n",
    "            t0 = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
