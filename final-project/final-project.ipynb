{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of fraudlent users in a Bitcoin user network #\n",
    "\n",
    "On platforms where users interact with each other to perform some type of financial transaction, it is very desirable to stop users who act in a fraudulent manner from being able to access or continue to use the platform. However, investigating users can be very expensive (consider e.g sellers on Ebay or Amazon) or impossible (consider users on a Bitcoin network) depending on the context. It is therefore worth asking the following - is it possible to perform accurate inference for the trustworthiness of users simply based on their behavior on the platform? We investigate this using publically available data for a Bitcoin user platform, where we observe ratings between users (and the times at which these ratings are made), and only know for a small subset of users whether they are trustworthy or not. \n",
    "\n",
    "## Summary of data ## \n",
    "\n",
    "For the dataset we are considering, we have 5,881 nodes (corresponding to separate users) and 35,592 weighted edges (corresponding to ratings from one user to another), along with time-stamp data; for example, a subset of the data looks like the following:\n",
    "\n",
    "Source node | Target node | Rating | Time stamp\n",
    ":----------:|:-----------:|:------:|:---------:\n",
    "6 | 2 |\t4 | 1289241912\n",
    "6 | 5 | 2 | 1289241942\n",
    "1 | 15 | 1 | 1289243140\n",
    "4 | 3 | 7 | 1289245277\n",
    "13 | 16 | 8 | 1289254254\n",
    "13 | 10 | 8 | 1289254301\n",
    "7 | 5 | 1 | 1289362700\n",
    "2 | 21 | 5 | 1289370557\n",
    "\n",
    "We note that ratings are integer valued between -10 to 10 (going from not-trustworthy to trustworthy) - when we analyse the data, we will normalize these to lie in [0, 1], and we will ignore the fact that user ratings are given on an integer scale. This is done mostly for computational reasons, as it will be more convenient to model ratings given by users on a continuous scale rather than a discrete one. The measurements for the time stamps are given as measurements of the time in seconds after 'Epoch' (January 1st 1970) in which the ratings were made; although we could try and use this information more substanatially (and e.g try and model trustworthiness as a function of time), for now we will only use it to compute per-node summary statistics. \n",
    "\n",
    "We also have some notion of \"ground truth\" data for the users in the network - there are 136 users which are considered trustworthy, and 180 not trustworthy users (so we have ground truth data for 316 users in total). Note that this is only for a subset of the users, so we will use a semi-supervised approach to modelling our data. As our end goal is to make inference for the trustworthiness for users, we will split the ground truth data in half (by randomly selecting indices) to form training and test sets when we are performing semi-supervised or supervised learning. \n",
    "\n",
    "Why split the ground truth data? Note that we only have partial information as to whether users are trustworthy, yet our overall goal is to infer some statistic of trustworthiness for **every** user. We therefore keep some of our ground truth data to the side so that we can make fair comparisons between unsupervised, semi-supervised and supervised approaches. One last note: there is likely an imblance in the proportions of trustworthy/non-trustworthy users in the train/test sets, as compared to those in the whole population, and so this is something which we should be aware of when modelling our data.\n",
    "\n",
    "## A quick caveat on our notion of ground truth ## \n",
    "\n",
    "Given that our data is taken from an inherently anonymous platform, you may (rightfully) ask whether the notion of 'ground-truth trustworthiness' comes from. For this dataset, the platform founder and those who they rated highly are considered to be trustworthy, and the users rated very lowly by the latter group are considered to be un-trustworthy. (In terms of the numerical specifics, I'm afraid I don't have those at hand - it's not very clear from the data source what exactly was done to obtain this information.) As a result, this is a clear limitation as to whether these users are actually acting fraudulently or not - instead we are really making inference for whether the platform holder would consider someone to be trustworthy. From e.g the perspective of the platform founder, this probably would suffice. \n",
    "\n",
    "## Evaluating model performance ## \n",
    "\n",
    "As our overall goal is to make inference for a latent state of each user (namely, whether they are trustworthy or not), the way in which we will evaluate our models will reflect this. In all of our models, we will try and infer for each user a probability of trustworthiness, as this is more informative than inference for the discrete state, and also provides us with more evaluation tools. To evaluate models, we will examine ROC curves and the AOC for labelled data in our training and test splits, along with calibration curves for the test data. Note that this only gives us a way of evaluating the model on a tiny proportion of our actual data set. We therefore will also examine:\n",
    "\n",
    "* the population distribution of the estimated probabilities of trustworthiness\n",
    "* posterior predictive checks for certain summary statistics of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "Before beginning to model the data in order to try and make inference for the trustworthiness of different users, we begin by exploring some of the features of our dataset to see what could potentially be informative for the purposes of achieving our overall goal. In particular, we're going to examine the following statistics of the data:\n",
    "\n",
    "* The in and out degree distribution (i.e, how many ratings a user has received and given respectfully)\n",
    "* The distribution of the mean and average ratings recived and given by users \n",
    "* The standard deviation of the time at which ratings are made\n",
    "* Rating asymmetry - the difference between the in and out degrees of a node, and simiarly the difference of the average in rating and out rating\n",
    "\n",
    "For the users whose ground truth state is available, we'll highlight their distribution in blue and orange (corresponding to trustworthy and not-trustworthy respectfully), and otherwise keep the overall distribution in grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Used to supress some NumPy warnings when loading data\n",
    "warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Used to automatically reload imported scripts for debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Import various external scripts with auto-reloading\n",
    "%aimport eda\n",
    "%aimport plotfns\n",
    "%aimport bayes\n",
    "%aimport bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to verify the output, switch the commenting of the following lines\n",
    "eda.plot_summary_stat(fname='img/summary_stat.png', display=False)\n",
    "# eda.plot_summary_stat(fname=None, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/summary_stat.png)\n",
    "\n",
    "Examining the above plots, we see that in terms of node-level information we can use to try and predict whether a user is trustworthy, we observe that there are a few discriminating features we could try and use:\n",
    "\n",
    "* **The average rating received** - Somewhat unsuprisingly, the average rating received by untrustworthy users is lower than that of trustworthy users, and lies on the left tail of the overall distribution of average ratings. \n",
    "* **The standard deviation of ratings given and received** - As observable from the two graphs, large observed values of the standard deviation of the ratings given or received by a user are more likely to come from users who have been deemed non-trustworthy, compared to both the base population and users who are considered trustworthy. For the ratings received, this is not too unsuprising - one can imagine that a collection of conspiring users who are not trustworthy will try and collectively give each other large positive ratings, while other users will give them large negative ratings. However, it is a bit more difficult to try and think of a reason as to why the standard deviation of the ratings given seems to vary across the different population.\n",
    "* **Standard deviation of rating times** - This appears to be a somewhat weaker form of evidence, but we note that a large proportion of un-trustworthy users are only active for a short period of time, whereas the distribution of trustworthy is more uniform (and similarly so for the overall population of users).\n",
    "\n",
    "Given this, we'll therefore begin by seeing what happens when using a simple logistic regression on the labelled part of the dataset, using these as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: A straightforward Bayesian logistic regression\n",
    "\n",
    "We'll begin by fitting the following Bayesian logistic regression model based off the examination above: we'll suppose that user $i$ has a trustworthy indicator $a_i$ with $a_i = 1$ corresponding to a user being trustworthy and $a_i = 0$ not trustworthy. We'll then suppose that the $a_i$ are independent Bernoulli random variables, whose probability of success $p_i$ conditional on \n",
    "\n",
    "* The average rating received $\\mathrm{R}_i$, centered to lie within $[-0.5, 0.5]$ (see footnote [a] for more info);\n",
    "* The standard deviation of ratings given $\\mathrm{SDG}_i$ and received $\\mathrm{SDR}_i$, rescaled to lie within $[0, 1]$ (see footnote [b] for more info); and\n",
    "* The standard deviation of rating times $\\mathrm{SDT}_i$ given, rescaled to lie within $[0, 1]$ (see footnote [c] for more info),\n",
    "\n",
    "is given by \n",
    "\n",
    "$$ \\log\\Big( \\frac{p_i}{1 - p_i} \\Big) = \\mu + \\beta_R \\cdot \\mathrm{R}_i + \\beta_{SDG} \\cdot \\mathrm{SDG}_i + \\beta_{SDR} \\cdot \\mathrm{SDR}_i + \\beta_{SDT} \\cdot \\mathrm{SDT}_i. $$\n",
    "\n",
    "Note that we want our model to accurately express \"in-difference\" for new users - if we have no information about a user, we should want the probability of a user being trustworthy to be (at least approximately) equal to the proportion of trustworthy users in the population. The easiest way of forcing this constraint is to ensure that i) for users for which we have no information about, all of the relevant covariates are equal to zero, and ii) we include an intercept term, which will hopefully capture the proportion of trustworthy users in the network. For users with no ratings, all of the relevant standard deviations can be set to zero, and similarly we can force the average rating received to be equal to zero for users with no ratings. We have also normalized the various variables so that they are on the same scale, as otherwise we will encounter difficulties when it comes to computation.\n",
    "\n",
    "In fitting the above model, we place independent $\\mathrm{Normal}(0, 1)$ priors on each of the non-intercept regression parameters (note that as we have normalized our covariates to be on the same scale, we also keep our prior variables on the same scale too), and a $\\mathrm{Normal}(0, 10)$ prior on the intercept parameter. As we can only fit the above model on the data points for which we have the ground truth labels and lie within our training set, we fit the described model using MCMC. \n",
    "\n",
    "------------\n",
    "\n",
    "[a] Given ratings in $[0, 1]$, we simply shift the values to lie in [-0.5, 0.5] via the mapping $x \\mapsto x - 0.5$.\n",
    "\n",
    "[b] It is is a relatively well known fact that for any random variable which takes values in $[0, 1]$, it's variance must lie in the interval $[0, 0.25]$, and hence the standard deviation must lie within $[0, 0.5]$. We therefore multiply the observed standard deviation by two in order to obtain values in $[0, 1]$. We also note that some of the standard deviations given are NaNs, in which case we set them equal to be equal to zero.\n",
    "\n",
    "[c] We handle the standard deviation of rating times in a slightly more complicated fashion than previously, as the empirical distribution of the rating times has a power law type distribution, meaning that simply diving by the observed maximum value will force most of the observed values to be very close to zero. While usually one would try and take logarithms to handle such data, we cannot do so naively as we have a large number of zeros (corresponding to users with either zero or one rating only). We therefore use a slight hack, and transform the variables according to $x \\mapsto \\log(x+1)$, and then divide by the observed maximum value after performing this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█████████████████████████████████| 2000/2000 [08:45<00:00,  4.38it/s, step size=3.46e-01, acc. prob=0.932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: mu\n",
      "       mean       std        5%       25%       50%      75%       95%\n",
      "0 -0.661118  0.464119 -1.438056 -0.968598 -0.642412 -0.34008  0.063848 \n",
      "\n",
      "Site: beta_r\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  4.551964  0.721617  3.360913  4.033078  4.579093  5.032453  5.750982 \n",
      "\n",
      "Site: beta_sdg\n",
      "       mean       std        5%       25%       50%       75%      95%\n",
      "0  0.417589  0.740799 -0.823498 -0.070828  0.448421  0.939174  1.62195 \n",
      "\n",
      "Site: beta_sdr\n",
      "       mean       std        5%       25%       50%      75%     95%\n",
      "0 -1.964712  0.740563 -3.161231 -2.455048 -1.950916 -1.46623 -0.7717 \n",
      "\n",
      "Site: beta_sdt\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  2.659175  0.628426  1.615651  2.242766  2.648361  3.067611  3.691149 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comment out the appriopate line depending on testing/running full model\n",
    "# hmc_arg_dict = {'model_num': 1, 'num_samples': 50, 'warmup_steps': 10}\n",
    "hmc_arg_dict = {'model_num': 1, 'num_samples': 1000, 'warmup_steps': 1000}\n",
    "\n",
    "# Run MCMC to get draws from the posterior distribution for the regression coefs\n",
    "hmc_samples = bayes.bayes_logistic_reg(**hmc_arg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism for Model 1 ##\n",
    "\n",
    "We'll begin by looking at the ROC curves produced by looking at the classifier created by the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, transform covariates to make test set predictions/do posterior predictive checks\n",
    "data = bitcoin.BitcoinOTC()\n",
    "\n",
    "r = data.in_weight_avg - 0.5\n",
    "sdg = 2*data.out_weight_std\n",
    "sdr = 2*data.in_weight_std\n",
    "sdt = torch.log(data.rate_time_out_std + 1)\n",
    "sdt = sdt/sdt.max()\n",
    "gt = data.gt\n",
    "\n",
    "# Set all NaN values to zero\n",
    "sdg[sdg != sdg] = 0\n",
    "sdr[sdr != sdr] = 0\n",
    "sdt[sdt != sdt] = 0\n",
    "\n",
    "# Create 5*(num of samples) matrix of regression coefficients from hmc_samples\n",
    "reg_coefs = []\n",
    "for value in hmc_samples.values():\n",
    "    reg_coefs.append(value)\n",
    "    \n",
    "reg_coefs = np.array(reg_coefs)\n",
    "\n",
    "# Create (num of train/test points)*4 matrix of covariates \n",
    "tt_str_list = ['train', 'test']\n",
    "logits = dict.fromkeys(tt_str_list)\n",
    "\n",
    "for i, split in enumerate([data.nodes_train, data.nodes_test]):\n",
    "    # Create (number of data points)*5 matrix of covariates of numpy type\n",
    "    covs = np.array([np.ones(len(split)), r[split].numpy(), sdg[split].numpy(), \n",
    "                     sdr[split].numpy(), sdt[split].numpy()])\n",
    "    \n",
    "    # Multiply together to give (num of data points)*(num of samples) array\n",
    "    logits[tt_str_list[i]] = covs.transpose() @ reg_coefs\n",
    "    \n",
    "# Create dict of ground truth labels for test and training set\n",
    "gt_labels = {'train': gt[data.nodes_train].numpy(), 'test': gt[data.nodes_test].numpy()}\n",
    "           \n",
    "# Create/plot ROC curve for training and test set \n",
    "plotfns.plot_roc_curve(gt=gt_labels, logits=logits, fname='img/bayes_1_roc.png', display=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bayes_1_roc.png)\n",
    "\n",
    "Here, we use each draw from the posterior distribution to form logit scores for the probability of trustworthiness, calculate the ROC and the corresponding AOC, and then average over our draws from the posterior distribution. Looking at this curve, you may think that we effectively have a perfect classifier, and so our task is done. However, what we are really interested in is obtaining correctly calibrated probabilities of trustworthiness; that is, if we had e.g $100$ users who had estimated trustworthiness probability equal to (say) $0.8$, then approximately $80$ should be trustworthy. If we plot a calibration curve based off the test set data, we see that this isn't the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfns.plot_calibration_curve(gt=gt_labels, logits=logits, fname='img/bayes_1_calib_curve.png', display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bayes_1_calib_curve.png)\n",
    "\n",
    "For our probabilities to be correctly calibrated, they should lie close to the diagonal line - we can see that this is clearly not the case here.\n",
    "\n",
    "To see some other aspects for which the above approach leaves something to be desired, we plot the distribution of estimated trustworthiness probabilities across the population of users in the network:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = np.array([np.ones(len(r.numpy())), r.numpy(), sdg.numpy(), sdr.numpy(), sdt.numpy()])\n",
    "pop_logits = covs.transpose() @ reg_coefs\n",
    "\n",
    "plotfns.plot_logit_distribution(gt=gt, \n",
    "                                pop_logits=pop_logits, \n",
    "                                train_set=data.nodes_train, \n",
    "                                test_set=data.nodes_test, \n",
    "                                fname='img/bayes_1_logit_pop.png',\n",
    "                                display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bayes_1_logit_pop.png)\n",
    "\n",
    "Here the bold blue line corresponds to the empirical c.d.f produced using the posterior mean of the estimated logits. Tick marks are also displayed corresponding to the estimated probabilities for the users which lie in the training set (black lines) or test set (pink asterisks), plotted at $y = 0$ if they are not trustworthy, and $y = 1$ if they are trustworthy. In addition to the threshold probability being low (a more natural threshold would usually be $0.5$), we see that there is a vertical jump in the c.d.f at around $p= 0.4$. This corresponds to the users for which we have little information, and so we would expect this jump to occur at the proportion of trustworthy users in the total population. However, according to the threshold we have, we would expect approximately $10\\%$ of users to be not trustworthy, meaning we have a disparity between the two. \n",
    "\n",
    "The major shortcoming of this approach can be best described as follows: \n",
    "\n",
    "1. **Labelled sub-population is a 'biased' sample from the overall population** - For this logistic regression to make sense, it requires that the users for which we have labelled data are reflective of the population of users as a whole in the network. Given both the nature of which the ground truth data were collected (we effectively only identify the ground truth in a neighbourhood of the platform founder), and the plots we showed before, this is clearly not the case. \n",
    "\n",
    "2. **Requires labels for both trustworthy and not-trustworthy users** - This is self-explanatory - note that in practice, if users are anonymous, then identifying whether they are actually trustworthy or not may be effectively impossible. We'd therefore like a method which is either unsupervised, or could at least allow for only observing trustworthy users (for instance, users known to the platform holder externally could be verified to be trustworthy). Ideally, we'd like a method which is able to learn some underlying latent structure from just the observed edge information, which should be naturally correlated to whether users are trustworthy. We can either use this directly, or hope that by adding a small amount of labels (even just from one class) we'll be able to obtain naturally calibrated probabilities of trustworthiness.\n",
    "\n",
    "The second point is clearly something which will require a different approach. For the first, we'll quickly go through another regression model just to provide some more evidence that this we can't try and skirt this issue by being clever. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1.1: A Bayesian logistic regression with interactions #\n",
    "\n",
    "Here we try and fit a model of the form \n",
    "\n",
    "$$ \\log\\Big( \\frac{p_i}{1 - p_i} \\Big) = \\mu + \\beta_R \\cdot \\mathrm{R}_i + \\beta_{R:SDG} \\cdot \\mathrm{R}_i \\cdot \\mathrm{SDG}_i + \\beta_{R:SDR} \\cdot \\mathrm{R}_i \\cdot \\mathrm{SDR}_i + \\beta_{R:SDT} \\cdot \\mathrm{R}_i \\cdot \\mathrm{SDT}_i. $$\n",
    "\n",
    "with similar priors as to before, and we create the same three plots (the ROC curves, the calibration curve, and the distribution of probabilities within the population) as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█████████████████████████████████| 2000/2000 [02:12<00:00, 15.14it/s, step size=8.16e-01, acc. prob=0.860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site: mu\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  0.267619  0.213188 -0.102799  0.130024  0.263281  0.401211  0.626599 \n",
      "\n",
      "Site: beta_r\n",
      "       mean       std        5%       25%       50%       75%       95%\n",
      "0  5.197097  0.715985  4.010979  4.688606  5.197781  5.669159  6.390642 \n",
      "\n",
      "Site: beta_rsdg\n",
      "      mean       std        5%       25%       50%       75%       95%\n",
      "0  0.77271  1.002562 -0.836703  0.107625  0.825097  1.480012  2.417683 \n",
      "\n",
      "Site: beta_rsdr\n",
      "       mean       std        5%       25%       50%       75%      95%\n",
      "0  1.448138  0.946653 -0.080398  0.820101  1.429556  2.034969  3.06368 \n",
      "\n",
      "Site: beta_rsdt\n",
      "       mean      std        5%       25%       50%       75%       95%\n",
      "0  2.079651  0.94269  0.532981  1.469091  2.102837  2.668191  3.632288 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comment out the appriopate line depending on testing/running full model\n",
    "# hmc_arg_dict = {'model_num': 2, 'num_samples': 50, 'warmup_steps': 10}\n",
    "hmc_arg_dict = {'model_num': 2, 'num_samples': 1000, 'warmup_steps': 1000}\n",
    "\n",
    "# Run MCMC to get draws from the posterior distribution for the regression coefs\n",
    "hmc_samples = bayes.bayes_logistic_reg(**hmc_arg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism for Model 1.1 ##\n",
    "\n",
    "We'll begin with the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5*(num of samples) matrix of regression coefficients from hmc_samples\n",
    "reg_coefs = []\n",
    "for value in hmc_samples.values():\n",
    "    reg_coefs.append(value)\n",
    "    \n",
    "reg_coefs = np.array(reg_coefs)\n",
    "\n",
    "# Create (num of train/test points)*4 matrix of covariates \n",
    "tt_str_list = ['train', 'test']\n",
    "logits = dict.fromkeys(tt_str_list)\n",
    "\n",
    "for i, split in enumerate([data.nodes_train, data.nodes_test]):\n",
    "    # Create (number of data points)*5 matrix of covariates of numpy type\n",
    "    covs = np.array([np.ones(len(split)), \n",
    "                     r[split].numpy(), \n",
    "                     r[split].numpy()*sdg[split].numpy(), \n",
    "                     r[split].numpy()*sdr[split].numpy(), \n",
    "                     r[split].numpy()*sdt[split].numpy()])\n",
    "    \n",
    "    # Multiply together to give (num of data points)*(num of samples) array\n",
    "    logits[tt_str_list[i]] = covs.transpose() @ reg_coefs\n",
    "    \n",
    "# Create dict of ground truth labels for test and training set\n",
    "gt_labels = {'train': gt[data.nodes_train].numpy(), 'test': gt[data.nodes_test].numpy()}\n",
    "           \n",
    "# Create/plot ROC curve for training and test set \n",
    "plotfns.plot_roc_curve(gt=gt_labels, logits=logits, fname='img/bayes_2_roc.png', display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bayes_2_roc.png)\n",
    "\n",
    "As we can see, the ROC basically appears to be unchanged (this shouldn't be suprising). Looking now at the calibration curve..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotfns.plot_calibration_curve(gt=gt_labels, logits=logits, fname='img/bayes_2_calib_curve.png', display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bayes_2_calib_curve.png)\n",
    "\n",
    "... we see that although the probabilities seem to be symmetric around $0.5$, their calibration properties remain poor as before. Looking at the empirical distribution of all of the predicted logits if we extrapolate the above logistic regression to the entire population, we see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = np.array([np.ones(len(r.numpy())), r.numpy(), \n",
    "                 sdg.numpy()*r.numpy(), \n",
    "                 sdr.numpy()*r.numpy(), \n",
    "                 sdt.numpy()*r.numpy()])\n",
    "pop_logits = covs.transpose() @ reg_coefs\n",
    "\n",
    "plotfns.plot_logit_distribution(gt=gt, \n",
    "                                pop_logits=pop_logits, \n",
    "                                train_set=data.nodes_train, \n",
    "                                test_set=data.nodes_test, \n",
    "                                fname='img/bayes_2_logit_pop.png',\n",
    "                                display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bayes_2_logit_pop.png)\n",
    "\n",
    "Here we see that at least the probabilities seem to cover more of the range $[0, 1]$, although given where the jump in the distribution occurs (corresponding to users who we have little to no information about), this probability should occur around $0.85$, the proportion of users which appear to be trustworthy/not fraudlent. \n",
    "\n",
    "None of this should be suprising, so let's move onto something more principled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Trying to jointly learn user behavior and trustworthiness\n",
    "\n",
    "Let's now think a bit more about how we would expect trustworthy and non-trustworthy/fraudlent users to act in this network. For trustworthy users, we expect that\n",
    "\n",
    "* They will tend to vote other trustworthy users as being trustworthy\n",
    "* They will tend to vote trustworthy users as being untrustworthy.\n",
    "\n",
    "For un-trustworthy users, it is somewhat harder to try and reason as to how they would behave, as they may act in an adversarial manner - for example, they may try and act as a trustworthy user, or they may try and discredit actual trustworthy users by rating them negatively. This is backed up by some of the summary data we saw before, where the distribution of the per user standard deviation of ratings given is far more uniform than that of the overall population, or known trustworthy users. Moreover, it is possible that untrustworthy users may act in different ways (some may try and disguise themselves as trustworthy, some may try and cast trustworthy users as non-trustworthy users), and we should try and choose a model which allows us some degree of flexibility. \n",
    "\n",
    "We therefore posit a model which acts as a mix between a logistic regression for the trustworthiness indicator, and a matrix factorization model for the observed ratings between users. In summary, we attempt to map each user $i$ in the network to a point $\\omega_i$ in Euclidean space, with distance between pairs of points reflecting whether they rated each other positively, with trustworthy and non-trustworthy users (hopefully) being linearly seperated.  To do so, we form a likelihood\n",
    "\n",
    "$$ \\begin{aligned} \n",
    "p(e_1, \\ldots, e_N \\,|\\, \\omega_1, \\ldots, \\omega_N, \\beta, \\mu) & = \\prod_{i=1}^N \\mathrm{Bernoulli}\\Big( e_i \\,\\Big|\\, p_i = \\sigma\\big( \\mu + \\langle \\beta,  \\omega_i \\rangle \\big) \\Big) \\\\\n",
    "p\\big( \\{a_{ij} \\,:\\, i \\neq j \\} \\,|\\, \\omega_1, \\ldots, \\omega_N \\big) & = \\prod_{i \\neq j} \\mathrm{Normal}\\big( \\mathrm{logit}(a_{ij}) \\,\\big|\\, \\langle \\omega_i, \\omega_j \\rangle, \\tau_0^2 \\big) \n",
    "\\end{aligned} $$\n",
    "\n",
    "where we have priors on the $\\omega_i$, $\\beta$ and $\\mu$ as follows:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "p(\\omega_1, \\ldots, \\omega_N) & = \\prod_{i=1}^N \\mathrm{Normal}\\Big(\\omega_i \\,\\Big|\\, 0, \\sigma^2_0 I_d \\Big) \\\\\n",
    "p(\\beta) &= \\mathrm{Normal}\\big( \\beta \\,\\big|\\, 0, 4 I_d \\big) \\\\\n",
    "p(\\mu) &= \\mathrm{Normal}( \\mu \\,|\\, 0, 10 )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be thought of as a tradeoff between a matrix factorization model for the edge ratings, along with a logistic regression for the underlying class labels. Here the variances $\\sigma_0^2$, $\\tau_0^2$ and dimension $d$ are hyperparameters of the model which we will select before trying to perform (approximate) inference.\n",
    "\n",
    "To find the posterior $p(\\omega_1, \\ldots, \\omega_N, \\beta \\,|\\, e_i, a_{ij} )$, we will approximate it using the following variational family\n",
    "\n",
    "$$ \\begin{aligned} p(\\omega_1, \\ldots, \\omega_N, \\beta, \\mu) & = p(\\omega_1, \\ldots, \\omega_N) p(\\beta) p(\\mu) \\qquad \\text{ where }\\\\\n",
    "p(\\omega_1, \\ldots, \\omega_N ) & = \\prod_{i=1}^N \\mathrm{Normal}\\big( \\omega_i \\,\\big|\\, \\mu_i, \\tau^2 I_d \\big) \\\\\n",
    "p(\\beta) &= \\mathrm{Normal}\\big( \\beta \\,\\big|\\, \\beta_0, \\sigma_{\\beta}^2 I_d \\big) \\\\\n",
    "p(\\mu) &= \\mathrm{Normal}\\big( \\beta \\,\\big|\\, \\mu_0, \\sigma_{\\mu}^2 \\big)\n",
    "\\end{aligned} $$\n",
    "\n",
    "and optimize over $\\sigma_{\\beta}^2, \\sigma_{\\mu}^2, \\tau^2$ and the $\\mu_i \\in \\mathbb{R}^d$ for $i = 1, \\ldots, N$.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "* Our original model is invariant to all the $\\omega_i$ and $\\beta$ being transformed by the same orthogonal matrix - by using the variational approximation, we will be effectively choosing one representative from this equivalence class. Importantly, this does not interfere with our inference for $p_i$.\n",
    "* Here we have specified a model as if the network were entirely observed, which is not the case in practice. We therefore only include edges which have been actually observed; we note that this only leads to correct inference for the underlying latent states if the way in which edges are formed in the graph is uninformative for these once given the observed values of these edges. We will come back to this issue shortly, as it ties into how stochastic black box VI can be performed in these models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An aside: Stochastic Variational Inference for Network Data #\n",
    "\n",
    "Recall that in variational inference, the goal is to approximate a desired posterior distribution $p(\\mathbf{z} | \\mathbf{x})$ of some latent variables $\\mathbf{z}$ given our data $\\mathbf{x}$ using a variational family $q(\\mathbf{z} | \\lambda)$, with $\\lambda$ parameterizing our variational family. To try and do so in an automated fashion, we update $\\lambda$ via performing gradient ascent on the ELBO, whose gradient is \n",
    "\n",
    "$$ \\nabla_{\\lambda} \\mathrm{ELBO} = \\mathbb{E}_{\\mathbf{z} \\sim q(\\cdot|\\lambda) }\\Big[ \\nabla_{\\lambda} \\log q(\\mathbf{z} | \\lambda) \\big\\{ \\log p(\\mathbf{x}, \\mathbf{z}) - \\log q(\\mathbf{z} | \\lambda) \\big\\} \\Big]   $$\n",
    "\n",
    "In practice, we then usually make use of the two following ideas in order to perform computationally efficient inference - we approximate the expectation by drawing samples from $\\mathbf{z} \\sim q(\\cdot | \\lambda)$, and then we also try and approximate the log-likelihood and its derivative. For the latter, the approach of subsampling data points is very intuitive for when we are fitting data which either naturally arises as being conditionally independent (such as for data we may consider modelling through a traditional regression), or we can extract some statistics from our data which can then be modelled in the same way (consider for example a topic model given a set of documents, where all we use are counts of words from a group of documents). \n",
    "\n",
    "However, for certain types of structured data, some care and thought is required in trying to exploit subsampling ideas to approximate the log-likelihood. Consider the following model: we have a directed graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ without self loops, where $|\\mathcal{V}| = N$, and $\\mathcal{E}$ is the list of edges, and have\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "p(\\omega_1, \\ldots, \\omega_N) &= \\prod_{i=1}^N N(\\omega_i \\,|\\, 0, \\eta^2 I_d) \\\\\n",
    "p(\\{ a_{ij} \\,|\\, i \\neq j \\} \\,|\\, \\omega_1, \\ldots, \\omega_n) &= \\prod_{i \\neq j} \\mathrm{Bernoulli}\\big(a_{ij} \\,\\big|\\, \\sigma(\\langle \\omega_i, \\omega_j \\rangle) \\big)\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\eta^2$ is fixed, and $\\sigma(x) = e^x/(1+e^x)$ is the logistic function. This has log-likelihood\n",
    "\n",
    "$$\n",
    "p\\big( \\{ \\omega_i \\}_i, \\{ a_{ij} \\}_{i \\neq j} \\big) = \\sum_{i \\neq j} \\Big\\{ a_{ij} \\log \\sigma (\\langle \\omega_i, \\omega_j \\rangle) + (1 - a_{ij}) \\log\\big( 1 - \\sigma(\\langle u, v \\rangle ) \\big) \\Big\\} - \\sum_{i=1}^n \\frac{1}{2\\sigma^2} \\| \\omega_i \\|_2^2. \n",
    "$$\n",
    "\n",
    "If we now let $S(\\mathcal{G}) = (\\mathcal{V}', \\mathcal{E}')$ denoted a \"subsampled\" version of the network, where $\\mathcal{V}' \\subseteq \\mathcal{V}$ and $\\mathcal{E}'$ is a list of pairs of vertices $\\mathcal{E}'$ (note that this may necessarily not be a subset of $\\mathcal{E}$ of our original edge set), we could approximate the log-likelihood by \n",
    "\n",
    "$$\n",
    "p\\big( \\{ \\omega_i \\}_i, \\{ a_{ij} \\}_{i \\neq j} \\big) \\approx \\frac{ N(N-1)  }{ |\\mathcal{E}'| } \\sum_{(i, j) \\in \\mathcal{E}'} \\Big\\{ a_{ij} \\log \\sigma (\\langle \\omega_i, \\omega_j \\rangle) + (1 - a_{ij}) \\log\\big( 1 - \\sigma(\\langle u, v \\rangle ) \\big) \\Big\\} - \\frac{ N }{ |\\mathcal{V}'|   } \\sum_{i \\in \\mathcal{V}'} \\frac{1}{2\\sigma^2} \\| \\omega_i \\|_2^2. \n",
    "$$\n",
    "\n",
    "The question is then in selecting $S(\\mathcal{G})$ - for example, this can be chosen by performing random walks on the graph, or by selecting edges/nodes randomly. Vietch et al. highlights that the **choice of sampling scheme is a modelling decision** in that it effects what we recover from fitting models such as the one described above. In other words:\n",
    "\n",
    "> **For the purposes of Box's loop, we should take into account our choice of sampling scheme as part of our modeling step, moreso than as part of our inference step.**\n",
    "\n",
    "## Choices of sampling scheme we'll consider ##\n",
    "\n",
    "Although there is a range of sampling schemes we could potentially consider, as we only partially observe network edges (with the edges taking on continuous variables), we consider two straighforward examples to implement:\n",
    "\n",
    "1. **p-sampling** - We sample edges uniformly at random from the entirety of those observed with probability $p$.\n",
    "2. **Selecting edges by random walks** - A uniform random walk of length $K$ is performed on the undirected version of the observed network - from a node $i$, we move to the next via selecting an edge connected to $i$ uniformly at random - and then we remove any self-crossings from the graph. (Note: for reasons of implementation, this can only be used to explore nodes which have a positive in and out degree.)\n",
    "\n",
    "This specifies a collection of edges; the corresponding vertices will simply consisting only of those from which there is a sampled edge incident from it. Note that with these methods there is the potential for some observations to never be selected - this could be fixed for 1) by doing traditional mini-batching of the observed edges, and for 2) by shuffling an ordering of nodes and then going through this list sequentially to determine starting locations of nodes. Note that procedure 2) will select nodes (and thus incident edges) with higher degree more frequently than procedure 1), and so one can think that the choice of sampling procedure may possibly reflect some way of adjusting for the way in which edges are observed in the first place - we'll see if this substanatially effects our inference shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model 2 ##\n",
    "\n",
    "### Hyperparameter choice ###\n",
    "\n",
    "We begin by discussing the different hyperparameters and sampling schemes we will use when trying to fit the above model. Given that the overall goal is to making inference for a probability of a binary state, it seems safe to assume that the embedding behavior will be very low rank, and so we will restrict ourselves to setting $d \\in \\{4, 8\\}$. (It is usually customary to keep the dimension of the embedding vectors as powers of $2$.) The hyperparameter $\\sigma_0^2$ on the variance of the embedding vectors will be set to $1$; something which is more important to consider is the effect of the hyperparameter $\\tau_0^2$, as this controls the strength of the matrix factorization approximation, along with the balance between this term and the contributing linear regression term. We will therefore consider several different scales for $\\tau_0^2$, namely $\\tau_0^2 \\in \\{0.1, 1, 5 \\}$.\n",
    "\n",
    "### Sampling scheme choice ###\n",
    "\n",
    "We try the following sampling schemes with the following parameters:\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2+: Removing the non-negative definiteness assumption on $\\mathrm{logit}(a_{ij})$ #\n",
    "\n",
    "In the previous model, we tried to learn embedding vectors $\\omega_i$ such that $\\mathrm{logit}(a_{ij}) \\approx \\langle \\omega_i, \\omega_j \\rangle$ - this forces this matrix to be non-negative definite. To see how this is an implicit constraint on the behavior of users, consider a more simplisitic model where user $i$ gives rating $j$ either a ratings of $+1$ and $-1$, doing the latter with probability $p_{e_i, e_j}$. With the above assumption, this will require that the matrix \n",
    "\n",
    "$$ \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} $$\n",
    "\n",
    "is non-negative definite. This is equivalent to requiring that $p_{00} p_{11} \\geq p_{01} p_{10}$ - this places some constraint on the behavior of trustworthy and non-trustworthy users. We therefore investigate what happens if we allow for some degree of flexibility, by instead setting \n",
    "\n",
    "$$ p\\big( \\{a_{ij} \\,:\\, i \\neq j \\} \\,|\\, \\omega_1, \\ldots, \\omega_N \\big)  = \\prod_{i \\neq j} \\mathrm{Normal}\\big( \\mathrm{logit}(a_{ij}) \\,\\big|\\, \\langle \\omega_i, B \\omega_j \\rangle, \\tau_0^2 \\big) $$\n",
    "\n",
    "in the likelihood, where $B = \\mathrm{diag}\\big( I_{d/2}, - I_{d/2} \\big)$ - i.e, we allow for the logit ratings matrix to have negative eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
