{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of fraudlent users in a Bitcoin user network\n",
    "\n",
    "(give goal of project)\n",
    "\n",
    "## Summary of data\n",
    "\n",
    "(give description of data, along with some caveats, testing...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Black Box Variational Inference for Network Data\n",
    "\n",
    "Recall that in variational inference, the goal is to approximate a desired posterior distribution $p(\\mathbf{z} | \\mathbf{x})$ of some latent variables $\\mathbf{z}$ given our data $\\mathbf{x}$ using a variational family $q(\\mathbf{z} | \\lambda)$, with $\\lambda$ parameterizing our variational family. To try and do so in an automated fashion, we update $\\lambda$ via performing gradient ascent on the ELBO, whose gradient is \n",
    "\n",
    "$$ \\nabla_{\\lambda} \\mathrm{ELBO} = \\mathbb{E}_{\\mathbf{z} \\sim q(\\cdot|\\lambda) }\\Big[ \\nabla_{\\lambda} \\log q(\\mathbf{z} | \\lambda) \\big\\{ \\log p(\\mathbf{x}, \\mathbf{z}) - \\log q(\\mathbf{z} | \\lambda) \\big\\} \\Big]   $$\n",
    "\n",
    "In practice, we then usually make use of the two following ideas in order to perform computationally efficient inference:\n",
    "\n",
    "* **Approximating the expectation** - We draw samples from $\\mathbf{z} \\sim q(\\cdot | \\lambda)$ to give a Monte Carlo estimate of the above expectation;\n",
    "* **Approximating the log-likelihood** - To illustrate through an example, suppose we are in a regime where the likelihood for the data factors as $p(\\mathbf{x} | \\mathbf{z}) = \\prod_{i=1}^N p(x_i | z_i)$ - i.e we have $N$ data points $x_i$ which are conditionally independent given local latent variables $z_i$. If our number of data points $N$ is large, rather than compute $\\sum_{i=1}^N \\log p(x_i | z_i) + \\log(z_i)$, we instead draw a subset $I_M$ of $M$ data points and instead use the approximation\n",
    "$$ \\sum_{i=1}^N \\big\\{ \\log p(x_i | z_i) + \\log(z_i) \\big\\} \\approx \\frac{N}{M} \\sum_{i \\in I_M} \\big\\{ \\log p(x_i | z_i) + \\log(z_i) \\big\\}. $$\n",
    "This works well in practice even when $M \\ll N$, i.e when the number of subsampled data points is far smaller than the total number. \n",
    "\n",
    "Focusing on the latter idea for now, this approach of subsampling data points is very intuitive for when we are fitting data which either naturally arises as being conditionally independent (such as for data we may consider modelling through a traditional regression), or we can extract some statistics from our data which can then be modelled in the same way (consider for example a topic model given a set of documents, where all we use are counts of words from a group of documents). \n",
    "\n",
    "However, for certain types of structured data, some care and thought is required in trying to exploit ideas of subsampling to approximate the log-likelihood. Suppose we observe a undirected graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ where the number of nodes $|\\mathcal{V}| = N$, and $\\mathcal{E}$ is the list of edges; we can also equivalently think of having observed an adjacency matrix $A = (a_{ij})_{i, j \\in \\mathcal{V}}$. Some real-world examples of data collected in this format include:\n",
    "\n",
    "* **Social networks** - Here, users of the social network correspond to the vertices, and edges correspond to whether they are e.g friends on the network. You can imagine that we also have nodal information corresponding to characteristics of each user; for example, age, gender, geographical location, hobbies/interests and et cetera.\n",
    "* **Protein-protein interaction networks** - In these networks, we look at different proteins within a particular type of cell, and form a network with proteins as vertices, and edges denoting whether two proteins contribute to a shared biological function.\n",
    "* **User-trust networks** - Here, users correspond to vertices, with an edge from one vertex to the other corresponding to a user providing a rating, the edge label corresponding to a rating of trust one user gave to another.\n",
    "\n",
    "In the first two examples, we note that the presence of the edge between two vertices should be more likely if the two vertices have comparable latent features, and in the latter the distribution of the edge labels (that is, the ratings between users) should be different if a non-fraudlant user is rating a fruadlant user, as compared to a fraudlant user rating rating another fraudlant user. To summarize, for network data, while we care about making latent observations about each observed vertex, we need to do so using some amount of edge information. \n",
    "\n",
    "For such a network (supposing for now it is directed and without self-loops), we could imagine trying to form a Bayesian model of the form  \n",
    "$$ \\begin{aligned}\n",
    "p(\\omega_1, \\ldots, \\omega_N) &= \\prod_{i=1}^N N(\\omega_i \\,|\\, 0, \\eta^2 I_d) \\\\\n",
    "p(\\{ a_{ij} \\,|\\, i \\neq j \\} \\,|\\, \\omega_1, \\ldots, \\omega_n) &= \\prod_{i \\neq j} \\mathrm{Bernoulli}\\big(a_{ij} \\,\\big|\\, \\sigma(\\langle \\omega_i, \\omega_j \\rangle) \\big)\n",
    "\\end{aligned}$$\n",
    "where $\\eta^2$ is specified in advance, and $\\sigma(x) = e^x/(1+e^x)$ is the logistic function. We can then write the log-likelihood down as \n",
    "$$\n",
    "p\\big( \\{ \\omega_i \\}_i, \\{ a_{ij} \\}_{i \\neq j} \\big) = \\sum_{i \\neq j} \\Big\\{ a_{ij} \\log \\sigma (\\langle \\omega_i, \\omega_j \\rangle) + (1 - a_{ij}) \\log\\big( 1 - \\sigma(\\langle u, v \\rangle ) \\big) \\Big\\} - \\sum_{i=1}^n \\frac{1}{2\\sigma^2} \\| \\omega_i \\|_2^2. \n",
    "$$\n",
    "If we now let $S(\\mathcal{G}) = (\\mathcal{V}', \\mathcal{E}')$ denoted a \"subsampled\" version of the network, where $\\mathcal{V}' \\subseteq \\mathcal{V}$ and $\\mathcal{E}'$ is a list of pairs of vertices $\\mathcal{E}'$ (note that this may necessarily not be a subset of $\\mathcal{E}$ of our original edge set), we could approximate the log-likelihood by \n",
    "$$\n",
    "p\\big( \\{ \\omega_i \\}_i, \\{ a_{ij} \\}_{i \\neq j} \\big) \\approx \\frac{ N(N-1)  }{ |\\mathcal{E}'| } \\sum_{(i, j) \\in \\mathcal{E}'} \\Big\\{ a_{ij} \\log \\sigma (\\langle \\omega_i, \\omega_j \\rangle) + (1 - a_{ij}) \\log\\big( 1 - \\sigma(\\langle u, v \\rangle ) \\big) \\Big\\} - \\frac{ N }{ |\\mathcal{V}'|   } \\sum_{i \\in \\mathcal{V}'} \\frac{1}{2\\sigma^2} \\| \\omega_i \\|_2^2. \n",
    "$$\n",
    "\n",
    "We now need to address the question of how we choose $S(\\mathcal{G})$. Although there are some obvious first examples (e.g sample edges randomly), the work of Grover and Leskovec introduced \"node2vec\" [1], which uses biased random walks to help form $S(\\mathcal{G})$. Later work by Vietch et al. [2] highlights that the **choice of sampling scheme is a modelling decision**, with some on-going work (D. and Austern, [3]) making explicit how changing the sampling scheme effects the latent structure recovered (under the assumption that the graph is exchangable). The important take-away from this is as follows:\n",
    "\n",
    "> **For the purposes of Box's loop, we should take into account our choice of sampling scheme as part of our modeling step, moreso than as part of our inference step.**\n",
    "\n",
    "One final remark: so far we've been talking implicitly about using variational inference to approximate the posterior distribution. Beyond the inability to use subsampling methods for MCMC methods to obtain samples from the posterior, there is one other thing to note: given the form of the likelihood and prior on the $\\omega_i$ and $a_{ij}$ as above, the posterior will be _rotationally invariant_; that is, if $Q \\in \\mathbb{R}^{d \\times d}$ is an orthogonal matrix, then \n",
    "$$ Q\\omega_1, \\ldots, Q \\omega_N | a_{ij}, i \\neq j \\stackrel{\\text{d}}{=} \\omega_1, \\ldots, \\omega_N | a_{ij}, i \\neq j. $$\n",
    "As a consequence, the posterior mean (for example) of the $\\omega_i$ will be equal to zero, and so extracting sensible summary statistics from our learned embeddings will be far more difficult. (For those familar with MCMC methods, this can be thought of as being due to a lack of identifiability of the $\\omega_i$ in the above model, and so infact we would expect to have other computational difficulties in trying to sample from the posterior.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "(give initial description of what I'm going to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiGraph' object has no attribute '_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0c6fc5c1f3be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/otc_network.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ppp\\lib\\site-packages\\networkx\\classes\\graph.py\u001b[0m in \u001b[0;36mnumber_of_nodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \"\"\"\n\u001b[1;32m--> 782\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DiGraph' object has no attribute '_node'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "\n",
    "data = open(\"data/otc_network.csv\")\n",
    "#SOURCE, TARGET, RATING, TIME\n",
    "G = pickle.load(open(\"data/otc_network.pkl\", \"rb\"))\n",
    "G.number_of_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiGraph' object has no attribute '_adj'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-421caf7c0e49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_degree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ppp\\lib\\site-packages\\networkx\\classes\\digraph.py\u001b[0m in \u001b[0;36min_degree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \"\"\"\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mInDegreeView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ppp\\lib\\site-packages\\networkx\\classes\\reportviews.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, G, nbunch, weight)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbunch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_succ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_succ\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_succ\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pred\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_pred\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_succ\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnbunch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DiGraph' object has no attribute '_adj'"
     ]
    }
   ],
   "source": [
    "data.in_degree(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] - \n",
    "\n",
    "[2] - \n",
    "\n",
    "[3] - "
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
